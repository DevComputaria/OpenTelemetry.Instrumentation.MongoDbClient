# OpenTelemetry Testing Guidelines

This document outlines the testing standards and best practices for OpenTelemetry instrumentation libraries.

## General Testing Principles

- Write tests that are reliable, repeatable, and independent
- Follow the AAA pattern (Arrange, Act, Assert)
- Test public APIs thoroughly
- Keep tests simple and focused on a single behavior
- Use descriptive test names that explain what's being tested

## Test Organization

### Test Project Structure

- Use a separate test project for each library project
- Name test projects with `.Tests` suffix (e.g., `OpenTelemetry.Instrumentation.MongoDb.Tests`)
- Mirror the directory structure of the source code when organizing tests
- Group tests by the component they're testing

### Test Class Naming

- Name test classes after the class being tested with `Tests` suffix (e.g., `MongoDbClientInstrumentationTests`)
- For testing specific functionality across components, use descriptive names (e.g., `TracePropagationTests`)

### Test Method Naming

- Use a descriptive naming convention: `MethodName_Scenario_ExpectedResult`
- Prefer longer, more descriptive names over shorter, cryptic ones
- Include the expected behavior in the test name

Example:
```csharp
[Fact]
public void AddMongoDbInstrumentation_WithFilterOption_FiltersOutSpecifiedCommands()
```

## Unit Testing

### Testing Components

- Test each public API method
- Test different configurations and parameters
- Verify behavior with edge cases and error conditions
- Ensure backward compatibility

### Testing Instrumentation

- Verify that activities are created correctly
- Verify that attributes are set according to semantic conventions
- Test error handling and activity status setting
- Test filtering and enrichment capabilities

### Mocking Dependencies

- Use mocking frameworks (Moq, NSubstitute) for external dependencies
- Mock only what's necessary - prefer real implementations when practical
- Set up mocks with minimum required behavior
- Use strict mocks when appropriate to catch unexpected calls

Example:
```csharp
[Fact]
public void StartActivity_WithAllDataRequested_SetsCorrectAttributes()
{
    // Arrange
    var listenerMock = new Mock<ActivityListener>();
    listenerMock.Setup(l => l.ShouldListenTo(It.IsAny<ActivitySource>())).Returns(true);
    listenerMock.Setup(l => l.Sample(It.IsAny<ActivitySource>(), 
                                     It.IsAny<string>(), 
                                     It.IsAny<ActivityKind>(), 
                                     It.IsAny<IEnumerable<KeyValuePair<string, object>>>(), 
                                     It.IsAny<IEnumerable<KeyValuePair<string, object>>>()))
                .Returns(ActivitySamplingResult.AllDataAndRecorded);
    
    ActivitySource.AddActivityListener(listenerMock.Object);
    
    // Act
    var activity = MongoDbClientActivitySource.StartActivity("mongodb.query");
    
    // Assert
    Assert.NotNull(activity);
    Assert.Equal("mongodb.query", activity.OperationName);
    Assert.Equal(ActivityKind.Client, activity.Kind);
    Assert.True(activity.IsAllDataRequested);
}
```

## Integration Testing

### End-to-End Tests

- Test with actual dependencies when possible
- Verify complete instrumentation pipeline
- Test with different versions of the instrumented library
- Verify collected telemetry matches expectations

### Testing Configuration

- Test various configuration options
- Verify that configuration changes affect behavior correctly
- Test default configuration behavior

### Testing with In-Memory Exporters

- Use in-memory exporters to capture and verify telemetry
- Check both the presence and absence of expected data
- Verify attribute values and relationships

Example:
```csharp
[Fact]
public void MongoDbCommand_IsCorrectlyInstrumented()
{
    // Arrange
    using var memoryExporter = new InMemoryExporter<Activity>();
    using var tracerProvider = Sdk.CreateTracerProviderBuilder()
        .AddMongoDbClientInstrumentation()
        .AddProcessor(new SimpleActivityExportProcessor(memoryExporter))
        .Build();
    
    var mongoClient = new MongoClient("mongodb://localhost:27017");
    var database = mongoClient.GetDatabase("test");
    
    // Act
    var collection = database.GetCollection<BsonDocument>("test");
    collection.Find(Builders<BsonDocument>.Filter.Empty).ToList();
    
    // Assert
    var activity = memoryExporter.GetExportedItems().Single();
    Assert.Equal("mongodb.query", activity.DisplayName);
    Assert.Equal("mongodb", activity.GetTagValue(SemanticConventions.AttributeDbSystem));
    Assert.Equal("test", activity.GetTagValue(SemanticConventions.AttributeDbName));
    Assert.Equal("find", activity.GetTagValue(SemanticConventions.AttributeDbOperation));
}
```

## Test Utilities

### Helper Methods

- Create reusable test helpers for common operations
- Put complex setup logic in helper methods
- Create extension methods for common assertions

### Custom Test Infrastructure

- Create wrapper classes to simplify testing complex components
- Build fixture classes for expensive setup that can be reused
- Use `IClassFixture<T>` or `ICollectionFixture<T>` to share test context

Example:
```csharp
public class MongoDbTestFixture : IDisposable
{
    public MongoClient Client { get; }
    public IMongoDatabase Database { get; }
    
    public MongoDbTestFixture()
    {
        Client = new MongoClient("mongodb://localhost:27017");
        Database = Client.GetDatabase("test");
        
        // Set up test data
    }
    
    public void Dispose()
    {
        // Clean up test data
    }
}

public class MongoDbInstrumentationTests : IClassFixture<MongoDbTestFixture>
{
    private readonly MongoDbTestFixture _fixture;
    
    public MongoDbInstrumentationTests(MongoDbTestFixture fixture)
    {
        _fixture = fixture;
    }
    
    // Tests using _fixture
}
```

## Testing Semantic Conventions

- Verify that spans follow semantic convention guidelines
- Check that attribute names match the official specifications
- Test that attribute values are in the correct format
- Verify required vs optional attributes

## Testing Multi-Framework Support

- Test with different targeted framework versions when applicable
- Verify conditional compilation directives work correctly
- Test on different runtime environments

## Best Practices for OpenTelemetry-Specific Testing

### Testing Activities

- Test that activities are properly created
- Verify activity hierarchy (parent-child relationships)
- Check activity kind, name, and attributes
- Test activity status and event recording

### Testing Telemetry Collection

- Verify sampling behavior
- Test filters and enrichers
- Check resource detection and attribute propagation
- Verify proper shutdown and flushing behavior

### Testing Instrumentation Options

- Test default options behavior
- Verify custom options are correctly applied
- Test options validation

## Common Gotchas and Solutions

- Reset global state between tests to avoid test interference
- Be aware of static initializers and their impact on tests
- Account for asynchronous operations properly in tests
- Handle potential race conditions in activity processing

## Continuous Integration

- Run tests on multiple platforms (Windows, Linux, macOS)
- Test with multiple framework versions
- Verify backward compatibility
- Include code coverage reports

By following these guidelines, your tests will help ensure that OpenTelemetry instrumentation libraries are robust, maintainable, and adhere to the project's quality standards.
